<!DOCTYPE HTML>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Linear Complexity Randomized Self-attention Mechanism - Cookipedia</title>




<link href="../../css/style.css?v=2025-10-19" rel="stylesheet" type="text/css"/><link href="../../css/cookipedia.css?v=2025-10-19" rel="stylesheet" type="text/css"/><script data-lang="ja" data-mainpage="false" data-mathjax="true" data-repo="cookipedia" defer="True" id="app" src="../../funcs.js?v=2025-10-18"></script></head>
<body>
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item long-title">
<h1>Linear Complexity Randomized Self-attention Mechanism</h1>
<p><b>Linear Complexity Randomized Self-attention Mechanism</b> [1] とは、タイトルから推測する限り、random feature を用いてセルフアテンションの計算量を $\mathcal{O}(n)$ にした研究のようにみえる。であれば Performer 先輩（<a href="https://openreview.net/forum?id=Ua6zuk0WRH">Rethinking Attention with Performers</a>）とキャラが被るがおそらく何か違うのであろう。2022 年 4 月にアーカイブに発表された。</p>
<br/>
<h2>参考文献</h2>
<ol class="ref small">
<li>Lin Zheng, Chong Wang, Lingpeng Kong. Linear Complexity Randomized Self-attention Mechanism. arXiv preprint arXiv:2204.04667, 2022.<a class="note" href="https://arxiv.org/abs/2204.04667">[arXiv]</a></li>
</ol>
<br/>
<div class="categories">
<a href="../categories/transformer.html">Transformer</a>
</div>
</div>
</main>
</div>
</body>
</html>
