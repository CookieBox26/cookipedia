<!DOCTYPE HTML>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström Method - Cookipedia</title>
<link href="../../css/style.css?v=2025-10-21" rel="stylesheet" type="text/css"/><link href="../../css/cookipedia.css?v=2025-10-19" rel="stylesheet" type="text/css"/><script data-lang="ja" data-mainpage="false" data-mathjax="true" data-repo="cookipedia" defer="True" id="app" src="../../funcs.js?v=2025-10-18"></script></head>
<body>
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item long-title">
<h1>Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström Method</h1>
<p><b>Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström Method</b> [1] とは、セルフアテンションの計算コスト削減に<a href="nyström_method.html"> Nyström 近似</a>を応用した研究である。NeurIPS 2021 で発表された。なお、この研究に先立ちセルフアテンションの計算コスト削減に Nyström 近似を応用した研究として <b>Nyströmformer</b> [Xiong et al., 2021] があるが、Nyströmformer ではセルフアテンション行列（半正定値行列ではない）に直接 Nyström 近似を適用していたのに対し、本研究では セルフアテンション行列を半正定値行列に拡張してから Nyström 近似を適用しているのが異なる。 </p>
<br/>
<h2>参考文献</h2>
<ol class="ref small">
<li>Yifan Chen, Qi Zeng, Heng Ji, Yun Yang. Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström Method. In NeurIPS 2021. <a class="note" href="https://proceedings.neurips.cc/paper/2021/hash/10a7cdd970fe135cf4f7bb55c0e3b59f-Abstract.html">[Proceedings]</a></li>
<li><a href="https://github.com/pkuzengqi/Skyformer">https://github.com/pkuzengqi/Skyformer</a>（2022年3月29日参照）.</li>
<ul>
<li>モデルの実装のリポジトリである。動くコードがあるのと共に、プレ処理済み Long Range Arena へのリンクがある。</li>
</ul>
<li>福水 健次. カーネル法入門. 朝倉書店, 2010.</li>
<ul>
<li>15ページ（初版第7刷）に正定値カーネルの例がある。</li>
</ul>
<li><a href="https://cookie-box.hatenablog.com/entry/2022/03/31/125645">https://cookie-box.hatenablog.com/entry/2022/03/31/125645</a>（2022年3月31日参照）.</li>
<ul>
<li>[1] の内容についての筆者の理解を記述している。</li>
</ul>
</ol>
<br/>
<h2>アイデア</h2>
<p>正定値カーネルでもなければ対称でもないソフトマックスによるセルフアテンションを、ガウシアンカーネルに置き換えて対称行列に拡張してカーネル法の土俵に持ち込んだ上で Nyström 近似を適用している。</p>
<h3>近似誤差の議論</h3>
<p>元のセルフアテンション行列とそれを近似したセルフアテンション行列の差の行列のスペクトルノルムが上から抑えられることを示している（Theorem 2）。</p>
<h2>コード</h2>
リポジトリ [2] をクローンして以下を実行することができる。
<pre class="python">
from models.model_LRA import ModelForSC, ModelForSCDual
from config import Config

model_config = Config["lra-text"]["model"]
model_config["mixed_precision"] = True
model_config["attn_type"] = "softmax"
model = ModelForSC(model_config)
print(model)

import torch

x = torch.tensor([[0, 1, 2, 3, 4]])
label = torch.tensor([0])
y = model(x, None, label)
print(y)
</pre>
<div class="categories">
<a href="../categories/transformer.html">Transformer</a>
</div>
</div>
</main>
</div>
</body>
</html>
