<!DOCTYPE HTML>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>Transformer - Cookipedia α-version</title>
<link href="../css/style.css?v=2025-12-09" rel="stylesheet" type="text/css"/>
<link href="../css/cookipedia.css?v=2025-12-09" rel="stylesheet" type="text/css"/>
<script data-mathjax="true" data-repo="cookipedia" defer="true" id="app" src="../funcs.js?v=2025-12-14"></script>
</head>
<body>
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item">
<h1>Transformer</h1>
<p><b>Transformer</b> とは、入力である単語ベクトルの列（文章）から文脈を反映した特徴ベクトルの列を出力するためのニューラルモデルである。無論、入力は単語列でなくとも構わない。</p>
<br/>
<!--
<p>${\rm SM}(q, k) \equiv \exp(q \cdot k / \sqrt{p})$ は正定値カーネルである [3]。
</p>
<div style="overflow: auto">
$$
$$
</div>

<h2>参考文献</h2>
<ol class="ref">
    <li>Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention. In NeurIPS 2021. <a class="note" href="https://proceedings.neurips.cc/paper/2021/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html">[Proceedings]</a></li>
    <li>Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention. In NeurIPS 2021. <a class="note" href="https://proceedings.neurips.cc/paper/2021/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html">[Proceedings]</a></li>
</ol>
<br/>


<h2>セルフアテンション行列の計算量削減</h2>
<br/>

<h2></h2>


<h2>注釈</h2>
<ol class="note">
<li id="note1">
</li>
</ol>
<br/>
-->
<div class="categories">
<a href="../categories/transformer.html">Transformer</a>
</div>
</div>
</main>
</div>
</body>
</html>
