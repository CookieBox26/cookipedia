<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Transformer - Cookipedia</title>
<script src="../mathjax-config.js" defer></script>
<script type="text/javascript" id="MathJax-script" defer
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<link rel="stylesheet" type="text/css" href="../style.css">
<script src="../funcs.js"></script>
</head>
<body onload="init()">
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item">
<h1>Transformer</h1>
<p><b>Transformer</b> とは、入力である単語ベクトルの列（文章）から文脈を反映した特徴ベクトルの列を出力するためのニューラルモデルである。無論、入力は単語列でなくとも構わない。</p>


<br/>
<!--
<p>${\rm SM}(q, k) \equiv \exp(q \cdot k / \sqrt{p})$ は正定値カーネルである [3]。
</p>
<div style="overflow: auto">
$$
$$
</div>

<h2>参考文献</h2>
<ol class="ref">
    <li>Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention. In NeurIPS 2021. <a class="note" href="https://proceedings.neurips.cc/paper/2021/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html">[Proceedings]</a></li>
    <li>Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention. In NeurIPS 2021. <a class="note" href="https://proceedings.neurips.cc/paper/2021/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html">[Proceedings]</a></li>
</ol>
<br/>


<h2>セルフアテンション行列の計算量削減</h2>
<br/>

<h2></h2>


<h2>注釈</h2>
<ol class="note">
<li id="note1">
</li>
</ol>
<br/>
-->

<div class="categories">
<a href="../categories/category_transformer.html">Transformer</a>
</div>

</div>
</main>
</div>
</body>
</html>
