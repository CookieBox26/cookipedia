<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title></title>
<link rel="stylesheet" type="text/css" href="../style.css">
<script src="../funcs.js"></script>
</head>
<body onload="init()">
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item">
<h1>PyTorch</h1>

<h2>参考文献</h2>
<ol class="ref">
  <li>torch.utils.data &mdash; PyTorch 2.1 documentation. <a class="asis" href="https://pytorch.org/docs/stable/data.html"></a> (2023年10月8日参照).</li>
</ol>

<h2>CPU 版と GPU 版の違い</h2>
CPU 版と GPU 版で以下の実行結果が異なるのがわかる。これは CPU 版と GPU 版ではより低レイヤーでソートアルゴリズムが異なることに起因していると自分は推測しているが不明である。
<pre class="python">
import torch
x = torch.tensor([0.3, 0.2, 0.1, 0.4, 0.5])
print(x.topk(3, sorted=False))
</pre>
<pre class="console">
# GPU
torch.return_types.topk(
values=tensor([0.5000, 0.4000, 0.3000]),
indices=tensor([4, 3, 0]))
</pre>
<pre class="console">
# CPU
torch.return_types.topk(
values=tensor([0.4000, 0.5000, 0.3000]),
indices=tensor([3, 4, 0]))
</pre>

<h2>関数・クラス</h2>
<h3>torch.utils.data.DataLoader</h3>
データのロード順を制御するには、<pre class="inline">batch_sampler</pre>引数に、各バッチでロードすべきインデックスのリストを返すイテラブルを指定すればよい。
<pre class="python">
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class MyDataset(Dataset):
    def __init__(self, df):
        self.df = df
    def __len__(self):
        return len(self.df)
    def __getitem__(self, batch_idx):
        return self.df.loc[batch_idx, :].values

df = pd.DataFrame({
    'a': [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    'b': [1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101],
    'c': [2, 12, 22, 32, 42, 52, 62, 72, 82, 92, 102],
})
dataset = MyDataset(df)
dataloader = DataLoader(
    dataset=dataset,
    batch_sampler=[[0, 1, 4, 5], [2, 3, 6, 7], [8, 9, 10]],
)

for i_batch, data in enumerate(dataloader):
    print(f'----- batch {i_batch} -----')
    print(data)
</pre>
<pre class="console">
----- batch 0 -----
tensor([[ 0,  1,  2],
        [10, 11, 12],
        [40, 41, 42],
        [50, 51, 52]])
----- batch 1 -----
tensor([[20, 21, 22],
        [30, 31, 32],
        [60, 61, 62],
        [70, 71, 72]])
----- batch 2 -----
tensor([[ 80,  81,  82],
        [ 90,  91,  92],
        [100, 101, 102]])
</pre>

</div>
</main>
</div>
</body>
</html>
