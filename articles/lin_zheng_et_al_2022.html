<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Linear Complexity Randomized Self-attention Mechanism - Cookipedia</title>
<script src="../mathjax-config.js" defer></script>
<script type="text/javascript" id="MathJax-script" defer
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<link rel="stylesheet" type="text/css" href="../style.css">
<script src="../funcs.js"></script>
</head>
<body onload="init()">
<div class="container">
<div id="sidebar"></div>
<main class="main">
<div id="smartphone-header"></div>
<div class="item long-title">
<h1>Linear Complexity Randomized Self-attention Mechanism</h1>
<p><b>Linear Complexity Randomized Self-attention Mechanism</b> [1] とは、タイトルから推測する限り、random feature を用いてセルフアテンションの計算量を $\mathcal{O}(n)$ にした研究のようにみえる。であれば Performer 先輩（<a href="https://openreview.net/forum?id=Ua6zuk0WRH">Rethinking Attention with Performers</a>）とキャラが被るがおそらく何か違うのであろう。2022 年 4 月にアーカイブに発表された。</p>
<br/>


<h2>参考文献</h2>
<ol class="ref">
    <li>Lin Zheng, Chong Wang, Lingpeng Kong. Linear Complexity Randomized Self-attention Mechanism. arXiv preprint arXiv:2204.04667, 2022.<a class="note" href="https://arxiv.org/abs/2204.04667">[arXiv]</a></li>
</ol>
<br/>


<div class="categories">
<a href="../categories/category_transformer.html">Transformer</a>
</div>

</div>
</main>
</div>
</body>
</html>
